---
title: "Codework"
author: "Erik Schulte"
date: "28 4 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
#load required packages
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
library(readtext)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(ggplot2)
library(magrittr)
library(gdata)
library(kableExtra)
library(tidytext)
library(corpus)
library(tibble)


#load the UN Security Council Debates dataset
load("data/dataset.RData")
load("data/docs.RData")

#set a seed for reproducability
set.seed(2333)
#data wrangling

##rename common column to merge the content of the speeches
meta_speeches <- meta_speeches %>%
  rename(doc_id = filename)

## merge dataset raw with speeches
#mutate(meta_speeches, text = raw_docs$text)
#meta_speeches <- cbind(meta_speeches, raw_docs[c("text")])
meta_speeches <- merge(meta_speeches, raw_docs, by = 'doc_id')
##how to do it after a specific column???
#add_column(meta_speeches, raw_docs[c("text")], .after = "filename")
```

# make it a corpus
```{r}
#as_corpus_frame(meta_speeches$text, filter = NULL, row.names = NULL)
corp_meta_speeches <- corpus(meta_speeches, text_field = "text")
print(corp_meta_speeches)

summary(corp_meta_speeches, 1)
#is_corpus_frame(corp_meta_speeches$text)
#corpus
#corpus <- (meta_speeches)

#only the speeches from China/only corpus from CHina
#How do I assign the speeches to the columns (now the speeches are in the same order as in the document, but not tied to the countries)

corp_China <- corpus_subset(corp_meta_speeches, country == "China")
Chinaspeeches <- subset.data.frame(meta_speeches, country == "China")

#only creates value
#Germany <- meta_speeches$country == "Germany"
##make country names small (something wrong)
#chinaspeeches_lower <- corpus_subset(corp_meta_speeches, tolower(country) %in% tolower(corp_China))


## access to document level variables
head(docvars(corp_China))

##extract them the document level variables
docvars(corp_China, field = "speaker")

corp_China$year

##create new variables (does not work yet)
#corp_China$avg_speech_length <- (mean(corp_China$sentences))
#avg_speech_length
```

######
```{r}
#create a textcloud
library(tm)
library(textcloud)

corpus <- Corpus(VectorSource())
```
#######

```{r}
##separates by default into words all speeches from raw docs (here by sentences) variable after tokens
tidy_raw_sentences <- meta_speeches %>%
  unnest_tokens(sentences_content, text, token = "sentences" )  

##separate into words a new variable each word per speech (gives us 47,4 Mio obs.)
tidy_raw_words <- meta_speeches |> 
    unnest_tokens(word, text)

##remove redundant words (reduces from 47,6 Mio words to 21,4 Mio words)
data(stop_words)  ##this comes with a package (a list of unuseful words)

tidy_raw_words <- tidy_raw_words %>%
  anti_join(stop_words)

##count words, after redundant words were removed (only meaningful words)
tidy_raw_words %>%
  count(word, sort = TRUE)

##plotting most common words (mit mehr als 80.000 Wörtern)
library(ggplot2)

tidy_raw_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 80000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

```{r same for China}
##separates by default into words all speeches from raw docs (here by sentences)
tidy_raw_sentences_CHN <- Chinaspeeches %>%
  unnest_tokens(sentences_content, text, token = "sentences" )  

#How do I assign the speeches to the columns (now the speeches are in the same order as in the document, but not tied to the countries)

##separate into words
tidy_raw_words_CHN <- Chinaspeeches |> 
    unnest_tokens(word, text)

##remove redundant words (reduces 600k thousand words)
data(stop_words)  ##this comes with a package (a list of unuseful words)

tidy_raw_words_CHN <- tidy_raw_words_CHN %>%
  anti_join(stop_words)

##count words, after redundant words were removed (only meaningful words)
tidy_raw_words_China %>%
  count(word, sort = TRUE)

##plotting most common wordsChina (mit mehr als 3.000 Wörtern)
library(ggplot2)

tidy_raw_words_China %>%
  count(word, sort = TRUE) %>%
  filter(n > 3000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```
# Gutenberg R

```{r}
#calculate the frequency of terms that appear in each speech. It also creates a document feature matrix.
#frequency of each word in the corpus
# function takes the respective corpus as an input and returns the frequencies in the form of a data frame


get_freqs <- function(corp_meta_speeches, weighting = TRUE){
tokens <- tokens(corp_meta_speeches, remove_punct = T, remove_separators = T, include_docvars = T)
tokens <- tokens_remove(tokens, stopwords("en"))
dfm <- tokens %>% dfm()
freqs <- textstat_frequency(dfm)

if(weighting == TRUE){
freqs$term_frequency <- freqs$frequency / sum(freqs$frequency)
freqs$inverse_doc_freq <- log10( length(corp_meta_speeches) / freqs$docfreq )
freqs$frequency <- freqs$term_frequency*freqs$inverse_doc_freq
}
return(freqs)

}


freqs_CHN <- get_freqs(corp_China,weighting=TRUE)
freqs_all <- get_freqs(corp_meta_speeches,weighting = TRUE)

##why is the frequency in general a bit lower than in the tasks before with the plot where a plot a graph of the words? here it also includes Mr. etc....

#Mr. und Mrs. daran kann man vielleicht etwas zur Frauenquote sagen
```

# i have to remove ("interpretation from Chinese first actually)



```{r}
library(scales)

# expect a warning about rows with missing values being removed
ggplot(freqs_CHN, aes(x = frequency(), y = `Jane Austen`, 
                      color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```

getting dictionaries for my purpose into R
```{r}
library(SentimentAnalysis)

#Harvard General Inquiry Database (only positive and negative)
data(DictionaryGI)
summary(DictionaryGI)
##Loughlan and MacDonald
data(DictionaryLM)
summary(DictionaryLM)

data(DictionaryGI)
summary(DictionaryGI)
loadDictionaryLM()
loadDictionaryLM_Uncertainty()
#make dataframe from the list
as.data.frame(DictionaryGI) 


###another package to get this word list but not strong and weak modal..
library(edgar)
data(LMMasterDictionary)
summary(LMMasterdictionary)

# library(textdata)
# lexicon_loughran(
#   dir = NULL,
#   delete = FALSE,
#   return_path = FALSE,
#   clean = FALSE,
#   manual_download = FALSE
# )
```

```{r}
#set wd first (in R directly works better)
setwd(C:Users/Erik Schulte/ownCloud/Uni Göttingen/Stellenbosch University/Data Science Methods/Github Projects/Machine Learning Project/data)
#I import the loughran and mcdonald dictionary because it is suitable for my analysis, for sentiments

loughran <- data.frame(read_csv("Loughran-McDonald_MasterDictionary_1993-2021.csv"))
#loughran_raw <- data.frame(read_csv("Loughran-McDonald_MasterDictionary_1993-2021.csv"))

colnames(loughran) <- tolower(colnames(loughran))
loughran <- loughran[c( "word","positive","uncertainty","strong_modal","weak_modal")]
loughran$positive <- tolower(loughran$positive)
loughran$uncertainty <- tolower(loughran$uncertainty)
loughran$strong_modal <- tolower(loughran$strong_modal)
loughran$weak_modal <- tolower(loughran$weak_modal)


##merge mit dem anderen dictionary (was ich noch nicht habe...)
# dict <- cbindX(dict, loughran)
# colnames(dict) <- c("Military","Cooperation","Positive","Uncertainty","Strongmodal",
# "Weakmodal")
```
## Sentiment Analysis

```{r}
sentiment_analysis <- function (freqs_all, freqs_CHN, loughran){
data <- data.frame(matrix(NA,2,dim(loughran)[2]))  #data is the storage table
colnames(data) <- colnames(loughran)

for (i in colnames(loughran)){
join_1 <- freqs_all %>% inner_join(loughran, by= c("feature" = i))
a <- sum(join_1$frequency)/sum(freqs_all$frequency)*100

join_2 <- freqs_CHN %>% inner_join(loughran, by= c("feature" = i))
b <- sum(join_1$frequency)/sum(freqs_CHN$frequency)*100

data[i] <- rbind(a,b)
}
return(data)

}
```


```{r}
loughlan_sentiments <- get_sentiments("loughlan") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```


# thoughts from May, 6th 
Eventually think about to put the speech into a list and have one row per speech so it has the same lenght. May be easier to use algorithms etc...
