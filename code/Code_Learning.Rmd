---
title: "Codework"
author: "Erik Schulte"
date: "28 4 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
#load required packages
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
library(readtext)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(ggplot2)
library(magrittr)
library(gdata)
library(kableExtra)
library(tidytext)
library(corpus)
library(tibble)
library(readxl)

#load the UN Security Council Debates dataset
load("data/dataset.RData")
load("data/docs.RData")

#set a seed for reproducability
set.seed(2333)
#data wrangling

##rename common column to merge the content of the speeches
meta_speeches <- meta_speeches %>%
  rename(doc_id = filename)

## merge dataset raw with speeches
#mutate(meta_speeches, text = raw_docs$text)
#meta_speeches <- cbind(meta_speeches, raw_docs[c("text")])
meta_speeches <- merge(meta_speeches, raw_docs, by = 'doc_id')
##how to do it after a specific column???
#add_column(meta_speeches, raw_docs[c("text")], .after = "filename")
```

# make it a corpus
```{r}
#as_corpus_frame(meta_speeches$text, filter = NULL, row.names = NULL)
corp_meta_speeches <- corpus(meta_speeches, text_field = "text")
print(corp_meta_speeches)

summary(corp_meta_speeches, 1)
#is_corpus_frame(corp_meta_speeches$text)
#corpus
#corpus <- (meta_speeches)

#only the speeches from China/only corpus from CHina
#How do I assign the speeches to the columns (now the speeches are in the same order as in the document, but not tied to the countries)

corp_China <- corpus_subset(corp_meta_speeches, country == "China")
Chinaspeeches <- subset.data.frame(meta_speeches, country == "China")

#only creates value
#Germany <- meta_speeches$country == "Germany"
##make country names small (something wrong)
#chinaspeeches_lower <- corpus_subset(corp_meta_speeches, tolower(country) %in% tolower(corp_China))


## access to document level variables
head(docvars(corp_China))

##extract them the document level variables
docvars(corp_China, field = "speaker")

corp_China$year

##create new variables (does not work yet)
#corp_China$avg_speech_length <- (mean(corp_China$sentences))
#avg_speech_length
```

######
```{r}
#create a textcloud
library(tm)
library(textcloud)

corpus <- Corpus(VectorSource())
```
#######

```{r}
##separates by default into words all speeches from raw docs (here by sentences) variable after tokens
tidy_raw_sentences <- meta_speeches %>%
  unnest_tokens(sentences_content, text, token = "sentences" )  

##separate into words a new variable each word per speech (gives us 47,4 Mio obs.)
tidy_raw_words <- meta_speeches |> 
    unnest_tokens(word, text)

##remove redundant words (reduces from 47,6 Mio words to 21,4 Mio words)
data(stop_words)  ##this comes with a package (a list of unuseful words)

tidy_raw_words <- tidy_raw_words %>%
  anti_join(stop_words)

##count words, after redundant words were removed (only meaningful words)
tidy_raw_words %>%
  count(word, sort = TRUE)

##plotting most common words (mit mehr als 80.000 Wörtern)
library(ggplot2)

tidy_raw_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 80000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

```{r same for China}
##separates by default into words all speeches from raw docs (here by sentences)
tidy_raw_sentences_CHN <- Chinaspeeches %>%
  unnest_tokens(sentences_content, text, token = "sentences" )  

#How do I assign the speeches to the columns (now the speeches are in the same order as in the document, but not tied to the countries)

##separate into words
tidy_raw_words_CHN <- Chinaspeeches |> 
    unnest_tokens(word, text)

##remove redundant words (reduces to 600k thousand words)
data(stop_words)  ##this comes with a package (a list of unuseful words)

tidy_raw_words_CHN <- tidy_raw_words_CHN %>%
  anti_join(stop_words)

##count words, after redundant words were removed (only meaningful words)
tidy_raw_words_CHN %>%
  count(word, sort = TRUE)

##plotting most common wordsChina (mit mehr als 3.000 Wörtern)
library(ggplot2)

tidy_raw_words_CHN %>%
  count(word, sort = TRUE) %>%
  filter(n > 3000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```
# Gutenberg R

```{r}
#calculate the frequency of terms that appear in each speech. It also creates a document feature matrix.
#frequency of each word in the corpus
# function takes the respective corpus as an input and returns the frequencies in the form of a data frame


get_freqs <- function(corp_meta_speeches, weighting = TRUE){
tokens <- tokens(corp_meta_speeches, remove_punct = T, remove_separators = T, include_docvars = T)
tokens <- tokens_remove(tokens, stopwords("en"))
dfm <- tokens %>% dfm()
freqs <- textstat_frequency(dfm)

if(weighting == TRUE){
freqs$term_frequency <- freqs$frequency / sum(freqs$frequency)
freqs$inverse_doc_freq <- log10( length(corp_meta_speeches) / freqs$docfreq )
freqs$frequency <- freqs$term_frequency*freqs$inverse_doc_freq
}
return(freqs)

}


freqs_CHN <- get_freqs(corp_China,weighting=TRUE)
freqs_all <- get_freqs(corp_meta_speeches,weighting = TRUE)

##why is the frequency in general a bit lower than in the tasks before with the plot where a plot a graph of the words? here it also includes Mr. etc....

#Mr. und Mrs. daran kann man vielleicht etwas zur Frauenquote sagen
```

# i have to remove ("interpretation from Chinese first actually)



```{r}
library(scales)

# expect a warning about rows with missing values being removed
ggplot(freqs_CHN, aes(x = frequency(), y = `Jane Austen`, 
                      color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```

getting dictionaries for my purpose into R

```{r}
#set wd first (in R directly works better)
setwd(C:Users/Erik Schulte/ownCloud/Uni Göttingen/Stellenbosch University/Data Science Methods/Github Projects/Machine Learning Project/data)

#I manually collected the word from the Harvard General Inquirer for these wordlists
# Difficult, to obtain...

GI_dic <- data.frame(read_excel("General_inquire.xlsx"))
GI_dic$military <- tolower(GI_dic$military) #make words to lower case in military columns
GI_dic$powercoop <- tolower(GI_dic$powercoop) #mark words to lower case in powercoop column
colnames(GI_dic) <- tolower(colnames(GI_dic))

dict <- GI_dic |> 
    select(military, powercoop)  #I only use the word lists from military and powercoop

#dict <- GI_dic[c("military","powercoop")]  ##this would create a new dataframe
```


```{r}
loughran <- data.frame(read_excel("LoughranMcDonald_SentimentWordLists_2018.xlsx"))
#I import the loughran and mcdonald dictionary because it is suitable for my analysis, for sentiments


colnames(loughran) <- tolower(colnames(loughran))
# Select the following 4 categories for my analysis, exclude negative inter alia
loughran <- loughran[c("positive","uncertainty","strongmodal","weakmodal")]
# make all words to lower case
loughran$positive <- tolower(loughran$positive)
loughran$uncertainty <- tolower(loughran$uncertainty)
loughran$strongmodal <- tolower(loughran$strongmodal)
loughran$weakmodal <- tolower(loughran$weakmodal)


#cbindX = column-binds objects with different number of rows. 
dict <- cbindX(dict, loughran)
colnames(dict) <- c("Military","Cooperation","Positive","Uncertainty","Strongmodal",
"Weakmodal")
```

## Sentiment Analysis

```{r}
sentiment_analysis <- function (freqs_all, freqs_CHN, dict){
data <- data.frame(matrix(NA,2,dim(dict)[2]))  #data is the storage table
colnames(data) <- colnames(dict)

for (i in colnames(data)){
join_1 <- freqs_all %>% inner_join(dict, by= c("feature" = i))
a <- sum(join_1$frequency)/sum(freqs_all$frequency)*100

join_2 <- freqs_CHN %>% inner_join(dict, by= c("feature" = i))
b <- sum(join_1$frequency)/sum(freqs_CHN$frequency)*100

data[i] <- rbind(a,b)
}
return(data)

}

# important function called sentiment_analysis uses this dict data frame together with
# the previously created term frequency tables as inputs. It then calculates a proportional
# count of each category to measure the tone of the language and outputs the result as a data
# frame. To do so, the function initializes a storage table for our results named data. The
# column names of the result table are set to the categories contained in the dictionary table.
# We then iterate through the column names (i.e. the categories) to match the words of each
# category with the term frequency tables. The resulting data frame join_1 contains only
# the words, that are in both data frames (this is called an inner join). We then calculate
# the share of words for each category from the overall frequency. The same is applied to
# the second term frequency table and then saved to the result table.

```


```{r}
##also create a tidy dictionary, we I have 863 words with 5 sentiment categories in total
tidy_dict <- dict %>% tidyr::gather("Sentiment", value, 1:6) |> 
    rename(word = value) |> 
    drop_na()
```


```{r}
#https://www.tidytextmining.com/sentiment.html
# consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words (maybe that's I have one observation per row)
# lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment

# My lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, Military ,Cooperation, UNcertainty , strongmodal and weakmodal, anger,
# 
# Dictionary-based methods like the ones we are discussing find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text. 
# . It is important to keep in mind that these methods do not take into account qualifiers before a word, such as in “no good” or “not true”; a lexicon-based method like this is based on unigrams only.

# Now that the text is in a tidy format with one word per row, we are ready to do the sentiment analysis. First, let’s use the my self-created dictionary and filter() for the cooperation XXX words

```


```{r}
#what are my hypotheses?

# Hypothesis 1: China will hold more and longer speeches due to underpin its underpin as global power.

#Hypothesis 2:  Small countries use more words that fall in the category of Positive, Strong Modal, Cooperation and less Uncertainty Military and Weak modal 
#1 China uses more aggressive and and confident language

#2 Over time Chinas speech length increases as part of its hunger for power

```


##Sentiment analysis (Important Section)
```{r}
#CHINA ### try to do sentiment as in tidy text with R book (WORKS!!!!)
tidy_raw_words_CHN %>%
  inner_join(tidy_dict) %>%
  count(word, sort = TRUE)
```


```{r}
###same for the whole corpus
tidy_raw_words %>%
  inner_join(tidy_dict) %>%
  count(word, sort = TRUE)
```






## Descriptive Statistics
```{r}
desc <- data.frame(matrix(NA,3,3))
colnames(desc) <- c("Speeches","Sentences","Share")
rownames(desc) <- c("Corpus", "All States","China")
desc$Speeches <- rbind(length(corp),length(corp_1),length(corp_2))
desc$Sentences <- rbind(mean(summary(corp)$Sentences),mean(summary(corp_1)$Sentences),
mean(summary(corp_2)$Sentences))
desc$Share <- rbind(100, round((length(corp_1)/length(corp))*100,2),
round((length(corp_2)/length(corp))*100,2))
kable(desc, caption="\\label{tab:tab1}Descriptive Statistics of the UNSC Speech Data",
booktabs = T) %>% kable_styling(latex_options = "HOLD_position", font_size = 10)
```


# tidy text analysis I would need to make my dictionary in a tidy format







```{r}
loughlan_sentiments <- get_sentiments("loughlan") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```


# thoughts from May, 6th after talk to Pablo
Eventually think about to put the speech into a list and have one row per speech so it has the same length. May be easier to use algorithms etc...
Try to keep it simple.
