---
title: "Geopolitical Changes in the UNSC -- Is China's growing global ambitions reflected in its communication in the UNSC?"
author: "Erik Schulte"
date: "July 7th 2022"
bibliography: Tex/ref.bib
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction \label{Introduction}

References are to be made as follows: @fama1997[p. 33] and @grinold2000 Such authors could also be referenced in brackets [@grinold2000] and together [@fama1997 \& @grinold2000]. Source

Theory Part...

Based on the theory of China's potential shift in policy objectives in the UNSC, I now conduct the empirical evaluation.
For this purpose, I ﬁrst describe the data preparation process. Secondly, I conduct
the actual computer assisted text analysis which presents several measures and examines
the hypotheses. 

# Data  {-}
First of all, I present all relevant data sources and explain essential features to get a
proper overview. Afterwards, I explain the process of loading the data into R and how
we process the raw text corpus, to obtain meaningful results.

The dataset comprises UN Security Council debates between January 1995 and December 2020 and was downloaded from @UNSC. The official meeting protocols are split into distinct speeches. For every speech, metadata regarding the speaker, the speaker's nation or affiliation, and the speaker's role in the meeting is given. The topic of the meeting is also given. In total, the corpus contains 82,165 speeches extracted from 5,748 meeting protocols. @UNSC also provide a codebook with the exact description of their workflow how to set-up the database.

The data are split in to a file containing all information about the UNSC meetings, one file about the metadata of the speeches and one file containing the whole text of the speeches.

## Data Sources

For our empirical analysis we use three main data sources. The ﬁrst two data sources are
used for the text analysis and the building of the subgroups. Data on the speeches of
the UNSC were obtained from @schonfeld2019. It contains 82165 speeches over
the years 1995 until 2017.

The other two data sources are word dictionaries for the sentiment analysis. It is crucial for the sentiment analysis, that the dictionaries are suitable for the specific context of analysis. For example, a word list developed for sociology may not be a good fit to analyze text data from for ﬁnance, and vice versa. The selection of the right word lists is therefore of uttermost importance to get meaningful and robust results @grimmer2013[pp. 274-275]. Hence, I carefully choose only certain categories from the dictionaries which are less prone to missclassiﬁcation. Two wordlists namely Military
and Cooperation are from the most widely used standard dictionary, the Harvard General
Inquirer (see @GeneralInquirer). The other four wordlists: Positive, Uncertainty, Strong Modal and Weak Modal are from @Loughran. Word categories that were specially adapted to the ﬁnancial context by Loughran and McDonald (2011), such as the negative wordlist, are neglected in my analysis.

## Data preparation
In the next step, I import the speech data into R from the data folder of my project file. To be able to process larger amounts of text, I need special packages in R. These include the quanteda, quanteda.textplot and quanteda.textstats. Other libraries like tidyverse, magrtittr, dyplr, ggplot2, readtext, and kableExtra are used to manipulate the data and plot it. We also set a seed, so that our results are reproducible.
```{r, echo=TRUE, warning=FALSE, message=FALSE}
#load required packages
library(tidyverse)
library(readtext)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(ggplot2)
library(magrittr)
library(gdata)
library(kableExtra)
library(tidytext)
library(dplyr)

#load the UN Security Council Debates dataset
load("data/dataset.RData")
load("data/docs.RData")

#set a seed for reproducability
set.seed(2333)
# View data
#head(meta_meetings)
#head(meta_speeches)
```


I then rename the filename column of the data frame containing the metadata of the speeches into doc_id in order to have a common identifier for merging with the actual text data from the speeches (raw_docs).
```{r}
##rename common column to merge the content of the speeches
meta_speeches <- meta_speeches %>%
  rename(doc_id = filename) |> 
   filter(participanttype != "The President")

## merge dataset raw with speeches
meta_speeches <- merge(meta_speeches, raw_docs, by = 'doc_id')
#*
##how to do it after a specific column???
#add_column(meta_speeches, raw_docs[c("text")], .after = "filename")
# meta_speeches |> 
# summarize(length_speeches = mean(sentences))
# summarize(mean_tokens = mean(tokens))
# summarize(mean_unique_tokens = mean(types))
# meta_speeches |> 
#     group_by(country)
#     count(as.numeric(meta_speeches$sentences))
#     arrange(desc(n))
# 
# str(meta_speeches)
```

In the next step I want to make a corpus from my data. The command corpus comes from the quanteda package. Hence, the text column of the meta_speeces dataframe are now a corpus. I ﬁlter out speeches from the president of the UNSC, as he or she mostly speaks on behalf of the organization and not for their respective country. This reduces the number of speeches by about 30,000 to 50,933.

```{r}
#as_corpus_frame(meta_speeches$text, filter = NULL, row.names = NULL)
corp_meta_speeches <- corpus(meta_speeches, text_field = "text")
summary(corp_meta_speeches, 1)
corp_meta_speeches <- corpus_subset(corp_meta_speeches, participanttype != "The President")
#*
#is_corpus_frame(corp_meta_speeches$text)
```


I also create a unique corpus just for the speeches held by China. I do this using the corpus_subset command from my previously defined corpus. I also take a subset of the meta_speeches dataframe with only the speeches from China. In total there are 3564 speeches by China in the dataframe, a total share of 3.82 % of all speeches. So there appear to be 1619 speeches from China as participant type president. So, the speeches where china speaks on behalf of the president are excluded.
```{r}
#How do I assign the speeches to the columns (now the speeches are in the same order as in the document, but not tied to the countries)

corp_China <- corpus_subset(corp_meta_speeches, country == "China")
Chinaspeeches <- subset.data.frame(meta_speeches, country == "China")
share_CHN_speeches <- 1945/50933
#*
#only creates value
#Germany <- meta_speeches$country == "Germany"
##make country names small (something wrong)
#chinaspeeches_lower <- corpus_subset(corp_meta_speeches, tolower(country) %in% tolower(corp_China))

## access to document level variables of the China corpus
head(docvars(corp_China))

##extract them the document level variables
#docvars(corp_China, field = "speaker")

#corp_China$year

##create new variables (does not work yet)
#corp_China$avg_speech_length <- (mean(corp_China$sentences))
#avg_speech_length
```

As a next step, I separate the whole speeches into sentences in tidy format. One column for every sentence of a speech. For that I use the unnest_tokens function from the tidytext package. This lets the the number of observations grow to 1,767,696 million. The problem here is that now after every salutation like "Mr.", so after every point the function creates a new sentence.
When we create a dataframe for every word as an observation, the number of observation grows to 43,534,652 million. Therefore, we must clean the data further before using them for analysis.
```{r}
##separates by default into words all speeches from raw docs (here by sentences) variable after tokens
tidy_raw_sentences <- meta_speeches %>%
  unnest_tokens(sentences_content, text, token = "sentences" )

##separate into words a new variable each word per speech (gives us 47,4 Mio obs.)
tidy_raw_words <- meta_speeches |> 
    unnest_tokens(word, text)

```

The tidytext package also come with a list of stopwords -- words that are not meaningful and that we want to exclude from the analysis. The stopwords list contains 1149 words. Our dataframe reduces to 19,623,316 million observations -- by more than a half. This allows me to start with a first analysis. I can plot the most frequent words used in the speeches of the UNSC. For that, I plot the most common words that were used more than 80,000 times in total. The threshold of 65,000 is subjectively chosen and is based on the graphical aestethics.
```{r, echo=TRUE, message=FALSE}
##remove redundant words (reduces from 47,6 Mio words to 21,4 Mio words)
data(stop_words)  ##this comes with a package (a list of unuseful words)

tidy_raw_words <- tidy_raw_words %>%
  anti_join(stop_words)

##count words, after redundant words were removed (only meaningful words)
# tidy_raw_words %>%
#   count(word, sort = TRUE)

```
## A first visual inspection of the most commonly used words

We can see from the graph that council, security and united are the most frequently used words. This gives are first overview of the most used words.

```{r, echo=FALSE, message=FALSE}
##plotting most common words (mit mehr als 80.000 Wörtern)
library(scales)
most_common_words_UNSC <- tidy_raw_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 65000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
scale_x_continuous(labels = comma_format(big.mark = ".",
                                           decimal.mark = ",")) +
  geom_col() +
  #  ggtitle = (label = "The most common words in the UNSC")+
  labs(y = NULL, title = "Most common words in the UNSC")
 
 most_common_words_UNSC
```
I also plot the 40 most common words from the UNSC in a wordcloud.
```{r, message=FALSE}
# Load the wordcloud package
library(wordcloud)

# Compute word counts and assign to word_counts
word_counts_UNSC <- tidy_raw_words %>% 
  count(word)

wordcloud(
  # Assign the word column to words
  word = word_counts_UNSC$word, 
  # Assign the count column to freq
  freq =word_counts_UNSC$n,
  scale=c(2,.4),
  max.words = 40,
  colors = "blue"
)
```
I then apply the same analysis for the data frame from the Chinese speeches.
```{r, include=FALSE}
#same analysis for Chinese speeches....
##separates by default into words all speeches from raw docs (here by sentences)
tidy_raw_sentences_CHN <- Chinaspeeches %>%
  unnest_tokens(sentences_content, text, token = "sentences" )  

#How do I assign the speeches to the columns (now the speeches are in the same order as in the document, but not tied to the countries)

##separate into words
tidy_raw_words_CHN <- Chinaspeeches |> 
    unnest_tokens(word, text)

##remove redundant words (reduces to 600k thousand words)
#data(stop_words)  ##this comes with a package (a list of unuseful words)

tidy_raw_words_CHN <- tidy_raw_words_CHN %>%
  anti_join(stop_words)

##count words, after redundant words were removed (only meaningful words)
tidy_raw_words_CHN %>%
  count(word, sort = TRUE)
```


```{r, echo=FALSE}
##plotting most common wordsChina (mit mehr als 3.000 Wörtern)

most_common_CHN <- tidy_raw_words_CHN %>%
  count(word, sort = TRUE) %>%
  filter(n > 2500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
    scale_x_continuous(labels = comma_format(big.mark = ".",
                                           decimal.mark = ",")) +
  geom_col() +
  labs(y = NULL, title = "Most common words used by China")
most_common_CHN
```

Additionally, I visualize the 40 most common words in a wordcloud using the wordcloud package.
```{r, message=FALSE}
# Load the wordcloud package
library(wordcloud)

# Compute word counts and assign to word_counts
word_counts_CHN <- tidy_raw_words_CHN %>% 
  count(word)

wordcloud(
  # Assign the word column to words
  word = word_counts_CHN$word, 
  # Assign the count column to freq
  freq =word_counts_CHN$n,
  scale=c(2,.4),
  max.words = 40,
  colors = "red"
)
```

## Frequency terms
Next, I calculate frequency terms. The function get_freqs creates a list of the frequency of terms that appear in each speech. The function takes the respective corpus as an input and returns the frequencies in the form of a data frame. In the ﬁrst step, it creates tokens from the text documents. Tokens are a sequence of elementary lexical components, in our case words. Punctuation, separators and stop words like “he”, “do”, or “if” are excluded as they have no meaning for the content and therefore represent no real added value for the analysis.
The dfm() function is then applied to the tokens to create the document feature matrix (dfm). As the dfm is a very sparse matrix containing the documents as rows and the terms as columns, I aggregate the data over all documents and sort the frequency of the terms in a descending order using the textstat_frequency() command. In addition, a special feature of the get_freqs function is the weighting parameter, which is set to FALSE per default. If it is being activated the frequencies are weighted according to the term-frequency inverse
document frequency (tf.idf) weighting scheme. The theory behind this term weighting
scheme is further explained in section B.3. In the end we apply the get_dfm function to
the two corpora and save the result in the variable freqs_all and freqs_CHN respectively.

# i have to remove ("interpretation from Chinese first actually)

```{r}
#calculate the frequency of terms that appear in each speech. It also creates a document feature matrix.
#frequency of each word in the corpus
# function takes the respective corpus as an input and returns the frequencies in the form of a data frame


get_freqs <- function(corp_meta_speeches, weighting = TRUE){
tokens <- tokens(corp_meta_speeches, remove_punct = T, remove_separators = T, include_docvars = T)
tokens <- tokens_remove(tokens, stopwords("en"))
dfm <- tokens %>% dfm()
freqs <- textstat_frequency(dfm)

if(weighting == TRUE){
freqs$term_frequency <- freqs$frequency / sum(freqs$frequency)
freqs$inverse_doc_freq <- log10( length(corp_meta_speeches) / freqs$docfreq )
freqs$frequency <- freqs$term_frequency*freqs$inverse_doc_freq
}
return(freqs)

}


freqs_CHN <- get_freqs(corp_China,weighting=TRUE)
freqs_all <- get_freqs(corp_meta_speeches,weighting = TRUE)

##why is the frequency in general a bit lower than in the tasks before with the plot where a plot a graph of the words? here it also includes Mr. etc....

#Mr. und Mrs. daran kann man vielleicht etwas zur Frauenquote sagen
```
We then get a new dataframe with each word, the frequency (percentage share of appearance of all words), the rank based on the frequency, the total document frequency of the words, the term_frequency and the inverse_term_frequency.


# Sentiment Analysis

For the sentiment analysis, I have to get dictionaries for my purpose into R. I choose to do a dictionary-based approach as I am a beginner in text analysis.  In accordance with processing of the speech data, we lower all dictionary terms so that the matching function is case sensitive.  Two wordlists namely Military and Cooperation are from the most widely used standard dictionary, the Harvard General Inquirer @GeneralInquirer. I manually collected this word list. It was difficult to obtain due to a restricted access. I create the new dataframe dict with these two categories after making all words to lower case letters.

```{r}
library(readxl)
GI_dic <- read_excel("~/ownCloud/Uni Göttingen/Stellenbosch University/Data Science Methods/Github Projects/Machine Learning Project/data/General_Inquire.xlsx")

GI_dic$military <- tolower(GI_dic$military) #make words to lower case in military columns
GI_dic$powercoop <- tolower(GI_dic$powercoop) #mark words to lower case in powercoop column
colnames(GI_dic) <- tolower(colnames(GI_dic))

dict <- GI_dic |> 
    select(military, powercoop)  
```

The second dictionary I am using is the one by @Loughran. From this I append the wordlists "Positive, "Uncertainty", "Strongmodal" and "Weakmodal" to my dict.

```{r}
loughran <- read_excel("~/ownCloud/Uni Göttingen/Stellenbosch University/Data Science Methods/Github Projects/Machine Learning Project/data/LoughranMcDonald_SentimentWordLists_2018.xlsx")


colnames(loughran) <- tolower(colnames(loughran))
# Select the following 4 categories for my analysis, exclude negative inter alia
loughran <- loughran[c("positive","uncertainty","strongmodal","weakmodal")]
# make all words to lower case
loughran$positive <- tolower(loughran$positive)
loughran$uncertainty <- tolower(loughran$uncertainty)
loughran$strongmodal <- tolower(loughran$strongmodal)
loughran$weakmodal <- tolower(loughran$weakmodal)

#cbindX = column-binds objects with different number of rows. 
dict <- cbindX(dict, loughran)
colnames(dict) <- c("Military","Cooperation","Positive","Uncertainty","Strongmodal",
"Weakmodal")
```

The second important function called sentiment_analysis uses this dict data frame together with the previously created term frequency tables as inputs. It then calculates a proportional count of each category to measure the tone of the language and outputs the result as a data frame. To do so, the function initializes a storage table for our results named data. The column names of the result table are set to the categories contained in the dictionary table. We then iterate through the column names (i.e. the categories) to match the words of each category with the term frequency tables. The resulting data frame join_1 contains only the words, that are in both data frames (this is called an inner join). We then calculate the share of words for each category from the overall frequency. The same is applied to the second term frequency table and then saved to the result table.



```{r}
sentiment_analysis <- function (freqs_all, freqs_CHN, dict){
data <- data.frame(matrix(NA,2,dim(dict)[2]))  #data is the storage table
colnames(data) <- colnames(dict)

for (i in colnames(data)){
join_1 <- freqs_all %>% inner_join(dict, by= c("feature" = i))
a <- sum(join_1$frequency)/sum(freqs_all$frequency)*100

join_2 <- freqs_CHN %>% inner_join(dict, by= c("feature" = i))
b <- sum(join_1$frequency)/sum(freqs_CHN$frequency)*100

data[i] <- rbind(a,b)
}
return(data)

}

```

My self-created tidy dictionary contains two columns with 863 words and 5 sentiment categories in total. 88 words belong to the Cooperation wordlist (e.g. "co-opeation"), 79 to the Military wordlist (e.g. "army"), 353 to the Positive wordlist (e.g. "accomplishment"), 19 to the strong modal wordlist (e.g. "undisputed"), 297 to the uncertainty wordlist (e.g. "ambiguity") and 27 to the weak modal wordlist (e.g. "apparently".
```{r, echo=FALSE}
##also create a tidy dictionary, we I have 863 words with 5 sentiment categories in total
tidy_dict <- dict %>% tidyr::gather("Sentiment", value, 1:6) |> 
    rename(word = value) |> 
    drop_na() 
#make a tibble out of it
as_tibble(tidy_dict) 
```

```{r, include=FALSE}
#
words_per_sentiment <- tidy_dict |>  
   count(Sentiment)
```


```{r}
sentiment_counts <- tidy_dict %>%  
  count(Sentiment) %>%  
 mutate(sentiment2 = fct_reorder(Sentiment, n)) 
ggplot(sentiment_counts, aes(x = sentiment2, y = n)) + 
  geom_col() + 
  coord_flip() + 
  labs( 
    title = "Sentiment Counts in my personalised dictionary", 
    subtitle = "Number of Counts for the six Sentiment Wordlists",
    x = "Sentiment", 
    y = "Counts" 
  )
```


## Join the dictionary with the speech data
I create a new dataframe using the inner_join function that appends the my sentiment dictionary in tidy format to the dataframe from China containing all words. I can then see that out of the 522,480 words used by China, 44,588 match to my dictionary (8.53%). I then mutate a new column which calculates the share of the sentiment used. For China, we can see that 38.07% of the sentiments fall into the cooperation wordlist, 10% in the military wordlist, 47.86% in the Positive sentiment wordlist, 3.28% in the uncertainty wordlist and very small shares in the strongmodal and weakmodal categories. What is apparent at first is that, altough the uncertainty wordlist has a multiple times higher share on the words in my dictionary, it is the reverse in the share of sentiments in China's speeches.

```{r}
sentiment_review_CHN <- tidy_raw_words_CHN |> 
    inner_join(tidy_dict)
sentiment_review_CHN |> 
    count(Sentiment) |> 
    mutate(share_sentiment_category = n/44588)

```
We can further see the most words that appear both in my dictionary and in China's speeches. The high share of the cooperation sentiment is driven by the very frequent use of the word "peace" (7095 times). This is followed by the words cooperation, stability, strenghten, achieve and progress in descending order.
```{r}
sentiment_review_CHN |> 
    count(word, Sentiment) |> 
    arrange(desc(n)) |> 
    head()
```


## visualizing sentiments

```{r}
#sentiment_review_CHN2 <- sentiment_review %>%  
 # filter(sentiment %in% c("positive", "negative"))  
word_counts_CHN2 <- sentiment_review_CHN %>%  
  count(word, Sentiment) %>%  
  group_by(Sentiment) %>% 
  top_n(10, n) %>%  
  ungroup() %>%  
  mutate( 
    word2 = fct_reorder(word, n) 
  )
```

In this graph, I visualized the sentiments of each of the six categories, so that we can see the most common words for each category.
```{r}
ggplot(word_counts_CHN2, aes(x = word2, y = n, fill = Sentiment)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ Sentiment, scales = "free") + 
  coord_flip() + 
  labs( 
    title = "Sentiment Word Counts China", 
    x = "Words" 
  )
```


## can we count sentiments by year?
The following gives a table of the counts of the sentiments by Year.

```{r}
sentiment_over_time_CHN <- sentiment_review_CHN |> 
    count(year, Sentiment) |>  #see how genius the spread command is
    spread(Sentiment, n) 
```
We can also plot this nicely in a graph, which shows the number of the sentiment words used over time for each of the six sentiment categories.


```{r, include=FALSE}
Military_CHN <- ggplot( 
  sentiment_over_time_CHN,  
  aes(x = year, y = Military, group = 1)
) + 
  geom_line(show.legend = FALSE, color = "red") + 
 # coord_flip() + 
  labs( 
    title = "Overall Sentiment by Year", 
    subtitle = "Sentiment over time: Military", 
    x = "Years", 
    y = "n" 
  ) 

Cooperation_CHN <- ggplot( 
  sentiment_over_time_CHN,  
  aes(x = year, y = Cooperation, group = 1)
) + 
  geom_line(show.legend = FALSE, color = "red") + 
 # coord_flip() + 
  labs( 
    title = "Overall Sentiment by Year", 
    subtitle = "Sentiment over time: Cooperation", 
    x = "Years", 
    y = "n" 
  ) 

Positive_CHN <- ggplot( 
  sentiment_over_time_CHN,  
  aes(x = year, y = Positive, group = 1)
) + 
  geom_line(show.legend = FALSE, color = "red") + 
 # coord_flip() + 
  labs( 
    title = "Overall Sentiment by Year", 
    subtitle = "Sentiment over time: Positive", 
    x = "Years", 
    y = "n" 
  ) 

Strongmodal_CHN <- ggplot( 
  sentiment_over_time_CHN,  
  aes(x = year, y = Strongmodal, group = 1)
) + 
  geom_line(show.legend = FALSE, color = "red") + 
 # coord_flip() + 
  labs( 
    title = "Overall Sentiment by Year", 
    subtitle = "Sentiment over time: Strongmodal", 
    x = "Years", 
    y = "n" 
  ) 

Uncertainty_CHN <- ggplot( 
  sentiment_over_time_CHN,  
  aes(x = year, y = Uncertainty, group = 1)
) + 
  geom_line(show.legend = FALSE, color = "red") + 
 # coord_flip() + 
  labs( 
    title = "Overall Sentiment by Year", 
    subtitle = "Sentiment over time: Uncertainty", 
    x = "Years", 
    y = "n" 
  ) 

Weakmodal_CHN <- ggplot( 
  sentiment_over_time_CHN,  
  aes(x = year, y = Weakmodal, group = 1)
) + 
  geom_line(show.legend = FALSE, color = "red") + 
 # coord_flip() + 
  labs( 
    title = "Overall Sentiment by Year", 
    subtitle = "Sentiment over time: Weakmodal", 
    x = "Years", 
    y = "n" 
  ) 

```


```{r, message=FALSE}
#one line of code to put the pictures in one graph
library(gridExtra)
CHN_sentiment_development <- grid.arrange(Cooperation_CHN, Military_CHN, Positive_CHN, Strongmodal_CHN, Uncertainty_CHN, Weakmodal_CHN, nrow=3, ncol=2)
```



# Topic Modelling
For topic modelling we need to create document term matrices (dtm). Sparsity of a matrix refers to how many zeros it contains. It should contain one row for each speech given by China.
With the cast_dtm commmand from the tidytext package, we can count each word in each speech and then creates a dtm from the word counts per speech.

```{r}
dtm_review_CHN <- tidy_raw_words_CHN %>%   #assign dtm to tidy_CHN_dataframe
  count(word, speech) %>%  
  cast_dtm(speech, word, n) |>    #Cast the word counts by speech into a DTM
    as.matrix()

dtm_review_CHN[1:4, 2000:2008] #this shows output for rows 1-4 and columns 2000-2008
```


\newpage

# References {-}

<div id="refs"></div>


